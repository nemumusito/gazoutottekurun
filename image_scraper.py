import os
import requests
from bs4 import BeautifulSoup
import time
import re
from urllib.parse import unquote, urlparse
import json
import gradio as gr
from PIL import Image
import io
import webview
import threading
import traceback
import logging

# ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š
logging.basicConfig(filename='scraper.log', level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

# å®šæ•°ã®å®šç¾©
ASPECT_RATIO_CHOICES = [
    "æŒ‡å®šãªã— â¬œ",
    "1:1 â¬›",
    "4:3 ğŸ”²",
    "16:9 ğŸ“º",
    "9:16 ğŸ“±"
]
BASE_FOLDER = "img"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
TIMEOUT = 30  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰

# ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒ•ãƒ©ã‚°
cancel_flag = threading.Event()

def sanitize_filename(filename):
    return re.sub(r'[\\/*?:"<>|]', "", filename)

def create_folder(base_folder, query):
    sanitized_query = sanitize_filename(query)
    folder_path = os.path.join(base_folder, sanitized_query)
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
    return folder_path

def parse_aspect_ratio(aspect_ratio):
    if aspect_ratio == "æŒ‡å®šãªã— â¬œ":
        return None
    match = re.search(r'(\d+):(\d+)', aspect_ratio)
    if match:
        return float(match.group(1)) / float(match.group(2))
    return None

def download_and_convert_image(url, folder, aspect_ratio, aspect_ratio_tolerance):
    if cancel_flag.is_set():
        return None
    try:
        response = requests.get(url, stream=True, timeout=TIMEOUT)
        if response.status_code == 200:
            content_type = response.headers.get('content-type', '').lower()
            if 'image' in content_type:
                image_data = response.content
                image = Image.open(io.BytesIO(image_data))
                
                # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®ãƒã‚§ãƒƒã‚¯
                if aspect_ratio is not None:
                    width, height = image.size
                    image_ratio = width / height
                    if abs(image_ratio - aspect_ratio) > aspect_ratio_tolerance:
                        logging.info(f"Aspect ratio mismatch: {url}")
                        return None

                # å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
                parsed_url = urlparse(url)
                original_filename = os.path.basename(parsed_url.path)
                filename = sanitize_filename(original_filename)
                
                # æ‹¡å¼µå­ã‚’webpã«å¤‰æ›´
                filename_without_ext, _ = os.path.splitext(filename)
                filename = f"{filename_without_ext}.webp"
                
                filepath = os.path.join(folder, filename)
                
                # åŒåãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if os.path.exists(filepath):
                    logging.info(f"File already exists, skipping: {filepath}")
                    return None
                
                # Convert and save as WebP
                image.save(filepath, 'WEBP')
                logging.info(f"Downloaded and converted: {filepath}")
                return filepath
            else:
                logging.warning(f"Not an image: {url}")
        else:
            logging.warning(f"Failed to download: {url}. Status code: {response.status_code}")
    except requests.Timeout:
        logging.error(f"Timeout occurred while downloading: {url}")
    except Exception as e:
        logging.error(f"Error downloading/converting {url}: {str(e)}")
    return None

def fetch_image_urls(search_url, headers):
    if cancel_flag.is_set():
        return []
    try:
        response = requests.get(search_url, headers=headers, timeout=TIMEOUT)
        if response.status_code != 200:
            raise Exception(f"Failed to fetch search results. Status code: {response.status_code}")
        
        soup = BeautifulSoup(response.text, 'html.parser')
        image_urls = []
        
        for img in soup.find_all('a', class_='iusc'):
            if cancel_flag.is_set():
                break
            try:
                m_content = json.loads(img.get('m', '{}'))
                img_url = m_content.get('murl')
                if img_url and img_url.startswith('http'):
                    image_urls.append(img_url)
            except json.JSONDecodeError:
                continue
            except Exception as e:
                logging.error(f"Error processing image URL: {str(e)}")
        
        return image_urls
    except requests.Timeout:
        logging.error("Timeout occurred while fetching image URLs")
        return []
    except Exception as e:
        logging.error(f"Error fetching image URLs: {str(e)}")
        return []

def scrape_images(query, num_images=10, aspect_ratio="æŒ‡å®šãªã— â¬œ", aspect_ratio_tolerance=0.2, progress=None):
    cancel_flag.clear()
    search_url = f"https://www.bing.com/images/search?q={query}&form=HDRSC2&first=1"
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }
    
    if not os.path.exists(BASE_FOLDER):
        os.makedirs(BASE_FOLDER)
    folder = create_folder(BASE_FOLDER, query)
    
    image_urls = fetch_image_urls(search_url, headers)
    if not image_urls:
        logging.error("No image URLs found")
        return []
    
    downloaded_images = []
    
    target_ratio = parse_aspect_ratio(aspect_ratio)
    
    if progress is not None:
        progress(0, desc="Downloading images")
    
    for i, img_url in enumerate(image_urls):
        if cancel_flag.is_set():
            break
        if len(downloaded_images) >= num_images:
            break
        
        logging.info(f"Attempting to download: {img_url}")
        filepath = download_and_convert_image(img_url, folder, target_ratio, aspect_ratio_tolerance)
        if filepath:
            downloaded_images.append(filepath)
            if progress is not None:
                progress((len(downloaded_images)) / num_images, desc=f"Downloaded {len(downloaded_images)} of {num_images}")
        else:
            logging.warning(f"Failed to download image {i+1}")
        
        time.sleep(1)  # 1ç§’å¾…æ©Ÿã—ã¦ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ã«ã™ã‚‹

    logging.info(f"Total images downloaded: {len(downloaded_images)}")
    return downloaded_images

def gradio_scrape_images(query, num_images, aspect_ratio, aspect_ratio_tolerance, progress=gr.Progress()):
    try:
        if not query.strip():
            raise ValueError("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        if num_images < 1 or num_images > 50:
            raise ValueError("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ç”»åƒã®æ•°ã¯1ã‹ã‚‰50ã®é–“ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        
        downloaded_images = scrape_images(query, num_images, aspect_ratio, aspect_ratio_tolerance, progress)
        if not downloaded_images:
            if cancel_flag.is_set():
                raise gr.Error("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
            else:
                raise Exception("ç”»åƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return downloaded_images
    except Exception as e:
        logging.error(f"Error in gradio_scrape_images: {str(e)}")
        logging.error(traceback.format_exc())
        raise gr.Error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

def cancel_download():
    cancel_flag.set()
    return "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚"

iface = gr.Interface(
    fn=gradio_scrape_images,
    inputs=[
        gr.Textbox(label="æ¤œç´¢ã—ãŸã„ç”»åƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"),
        gr.Slider(minimum=1, maximum=50, value=10, step=1, label="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ç”»åƒã®æ•°"),
        gr.Dropdown(
            choices=ASPECT_RATIO_CHOICES,
            value="æŒ‡å®šãªã— â¬œ",
            label="ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”"
        ),
        gr.Slider(minimum=0.1, maximum=0.5, value=0.2, step=0.1, label="ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®è¨±å®¹ç¯„å›²")
    ],
    outputs=gr.Gallery(label="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ"),
    title="ç”»åƒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«",
    description="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€é–¢é€£ã™ã‚‹ç”»åƒã‚’è‡ªå‹•çš„ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚ç”»åƒã¯WebPå½¢å¼ã§ä¿å­˜ã•ã‚Œã¾ã™ã€‚",
    allow_flagging="never",
)

iface.load(lambda: None, None, gr.Button("Clear"), outputs=iface.inputs)
cancel_button = gr.Button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«")
cancel_button.click(fn=cancel_download, inputs=None, outputs=gr.Textbox())

def run_gradio():
    iface.launch(share=True)

def run_webview():
    webview.create_window("ç”»åƒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«", "http://127.0.0.1:7860")
    webview.start()

if __name__ == "__main__":
    gradio_thread = threading.Thread(target=run_gradio)
    gradio_thread.start()
    
    # Gradioã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã™ã‚‹ã®ã‚’å°‘ã—å¾…ã¤
    time.sleep(5)
    
    run_webview()
